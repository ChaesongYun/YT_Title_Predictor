{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1WGKvCcXxtljwsQ81UC4oOfs8bNPX_YWW",
      "authorship_tag": "ABX9TyPPJosOJlbaVY2jygS8s95G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaesongYun/YT_Title_Predictor/blob/main/Youtube_Title_Predictor_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YEcobUfSaSfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tY7y6u3a1pV",
        "outputId": "c43228d9-4898-4a31-b8e1-3447558fa712"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 Fine Tuning\n",
        "\n",
        "클리닝이 끝난 데이터를 BERT 자연어처리 인공지능 모델을 이용해서 fine tuning 작업을 해준다.<br>\n",
        "구글에서 pre-tranined된 BERT 모델을 받아준 뒤 데이터를 맞춤 제작을 한다.\n",
        "<br>\n",
        "맞춤제작이 끝나면 그 모델 유튜브 제목 퀄리티를 예측할 수 있다!(●'◡'●)\n",
        "\n",
        "1. Pre-tranined tokenizer 다운 받기\n",
        "2. Data sampling. 데이터 섞기\n",
        "3. 데이터셋을 training용과 validation용으로 나누기\n",
        "4. 인풋 데이터 만들기\n",
        "5. 학습모델 모델링\n",
        "6. 모델 compile\n",
        "7. 모델 fit\n",
        "8. 모델 평가하기\n",
        "9. 모델 다운로드"
      ],
      "metadata": {
        "id": "skR-_pGGitM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 라이브러리 가져오기\n",
        "!pip install transformers # Hugging Face에서 가져온다\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# 선행된 Tokenizer 다운받기\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0Jwjk8OjkAM",
        "outputId": "de4e4d8b-1cc5-49ac-e702-16d662153777"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaned 데이터 가져오기\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/Youtube_Title_Predictor/cleaned_data.csv\", index_col=False)\n",
        "\n",
        "# 데이터 섞기\n",
        "dataset = dataset.sample(frac=1)\n"
      ],
      "metadata": {
        "id": "u7FGKC3cj6xK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset을 학습용과 점검용으로 나누기\n",
        "ten_perc = int(len(dataset) * 0.1)\n",
        "train_titles = np.array(dataset['Title'][:-ten_perc], dtype=str)\n",
        "train_labels = np.array(dataset['Target'][:-ten_perc])\n",
        "\n",
        "valid_titles = np.array(dataset['Title'][-ten_perc:], dtype=str)\n",
        "valid_labels = np.array(dataset['Target'][-ten_perc:])\n",
        "\n",
        "train_titles[:5], valid_titles[:5]"
      ],
      "metadata": {
        "id": "6diVRnoFkZIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0082d2c-d243-421a-9196-550066da7ddf"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['리트리버가 도끼눈을 뜨고 주인을 노려보는 귀여운 이유 ㅋㅋㅋ',\n",
              "        '사건 현장 범인은 사라지고 quot 네가 그랬지 quot 허스키는 억울해 JTBC 사건반장',\n",
              "        '털 안빠지는 대형견 종류 TOP 4 강아지',\n",
              "        '휴가철 급증하는 유기견 보호자에게 매몰차게 버려진 개들은 어떻게 살아갈까 구조된 강아지들의 운명 왜그러냥귀엽개',\n",
              "        '유기견 사건의 결말'], dtype='<U98'),\n",
              " array(['사나운 개 웃는 개 사모예드',\n",
              "        '피크타임 16마리 비숑을 위한 강형욱 훈련사의 특급 솔루션 성격 별 그룹 형성 개는훌륭하다 KBS 230529 방송',\n",
              "        '인형인줄 아기 강아지 입양했어요 2개월 포메라니안 ep 1',\n",
              "        '늑대를 닮은 진돗개 호피 무늬 칡개 앞에 뱀이 나타났다 KBS 굿모닝 대한민국 120612 방송',\n",
              "        '강아지에게 계속 뽀뽀했더니 진돗개'], dtype='<U94'))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 인풋 데이터 만들기(샘플)\n",
        "\n",
        "i = 35\n",
        "tokenized_data = tokenizer.encode_plus(train_titles[i],\n",
        "                                       add_special_tokens = True,\n",
        "                                       max_length = 30,\n",
        "                                       padding=True,\n",
        "                                       truncation=True)\n",
        "tokenized_data = dict(tokenized_data)\n",
        "labels = np.array(train_labels[i])\n",
        "\n",
        "print(tokenized_data)\n",
        "print(f\"The label is: {labels}\")\n",
        "print(tokenizer.decode(tokenized_data[\"input_ids\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGCN-e_ujmcB",
        "outputId": "dd30adff-020a-40cf-95da-8c35fccc5855"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 122, 27056, 124, 38631, 9246, 12692, 80607, 9708, 118797, 21789, 11287, 9854, 54305, 82034, 9708, 12092, 9380, 28911, 10739, 9524, 11664, 10112, 119229, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "The label is: 0\n",
            "[CLS] 1년에 3천 마리 이상의 진돗개가 태어난다는 진도 빅데이 알고e즘 [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_encode(titles, maximum_length):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  # token_type_ids = []\n",
        "\n",
        "  for title in titles:\n",
        "    encoded = tokenizer.encode_plus(title,\n",
        "                                    add_special_tokens = True,\n",
        "                                    max_length = maximum_length,\n",
        "                                    pad_to_max_length =True,\n",
        "                                    truncation=True\n",
        "                                    )\n",
        "    input_ids.append(encoded['input_ids'])\n",
        "    attention_masks.append(encoded['attention_mask'])\n",
        "    # token_type_ids.append(encoded['token_type_ids'])\n",
        "\n",
        "  return np.array(input_ids), np.array(attention_masks)"
      ],
      "metadata": {
        "id": "f_CURFAskteR"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_input_ids, train_attention_masks = bert_encode(train_titles, 30)\n",
        "valid_input_ids, valid_attention_masks = bert_encode(valid_titles, 30)\n",
        "\n",
        "train_input_ids.shape, train_attention_masks.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ksIe4IxlVca",
        "outputId": "df6a6cf0-eec8-45a3-d7e8-2656b75651f4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((792, 30), (792, 30))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def labels(data):\n",
        "  labels = []\n",
        "\n",
        "  for i in data:\n",
        "    labels.append(i)\n",
        "\n",
        "  return np.array(labels, dtype='int32')\n",
        "\n",
        "# print(train_labels)\n",
        "train_labels = labels(train_labels)\n",
        "valid_labels = labels(valid_labels)\n"
      ],
      "metadata": {
        "id": "maRlh0MjlrB3"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 만들기\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_model(bert_model):\n",
        "  input_ids = tf.keras.Input(shape=(30,), dtype='int32')\n",
        "  attention_masks = tf.keras.Input(shape=(30,), dtype='int32')\n",
        "  # token_type_ids = tf.keras.Input(shape=(30,), dtype='int32')\n",
        "\n",
        "  output = bert_model([input_ids, attention_masks])\n",
        "  output = output[1]\n",
        "  output = tf.keras.layers.Dense(32, activation='relu')(output)\n",
        "  output = tf.keras.layers.Dropout(0.2)(output)\n",
        "  output = tf.keras.layers.Dense(1, activation='sigmoid')(output)\n",
        "\n",
        "  model = tf.keras.models.Model(inputs = [input_ids, attention_masks], outputs=output)\n",
        "  model.compile(Adam(learning_rate=0.00001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "jHnBhWNlbt-8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 선행학습된 버트 모델 받기\n",
        "from transformers import TFBertModel\n",
        "\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-multilingual-cased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39Vv6JmwdQS8",
        "outputId": "98c1d8b2-148b-4077-d2eb-1a767945d43d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Summary\n",
        "model = create_model(bert_model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-oOovjVdbsm",
        "outputId": "9efe768b-04e5-4b3f-9b61-65af450923ce"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_13 (InputLayer)       [(None, 30)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)       [(None, 30)]                 0         []                            \n",
            "                                                                                                  \n",
            " tf_bert_model_5 (TFBertMod  TFBaseModelOutputWithPooli   1778534   ['input_13[0][0]',            \n",
            " el)                         ngAndCrossAttentions(last_   40         'input_14[0][0]']            \n",
            "                             hidden_state=(None, 30, 76                                           \n",
            "                             8),                                                                  \n",
            "                              pooler_output=(None, 768)                                           \n",
            "                             , past_key_values=None, hi                                           \n",
            "                             dden_states=None, attentio                                           \n",
            "                             ns=None, cross_attentions=                                           \n",
            "                             None)                                                                \n",
            "                                                                                                  \n",
            " dense_12 (Dense)            (None, 32)                   24608     ['tf_bert_model_5[0][1]']     \n",
            "                                                                                                  \n",
            " dropout_228 (Dropout)       (None, 32)                   0         ['dense_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_13 (Dense)            (None, 1)                    33        ['dropout_228[0][0]']         \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 177878081 (678.55 MB)\n",
            "Trainable params: 177878081 (678.55 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 학습하기\n",
        "history = model.fit([train_input_ids, train_attention_masks], train_labels,\n",
        "                    validation_split=0.2, epochs = 3, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0XDF9YxdhdI",
        "outputId": "114b09be-cb8b-4a26-cc7e-052060444b60"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "20/20 [==============================] - 5s 270ms/step - loss: 0.5718 - accuracy: 0.7077 - val_loss: 0.6551 - val_accuracy: 0.6478\n",
            "Epoch 2/3\n",
            "20/20 [==============================] - 5s 256ms/step - loss: 0.5445 - accuracy: 0.7409 - val_loss: 0.6487 - val_accuracy: 0.6478\n",
            "Epoch 3/3\n",
            "20/20 [==============================] - 6s 300ms/step - loss: 0.5059 - accuracy: 0.7694 - val_loss: 0.7171 - val_accuracy: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가하기\n",
        "model.evaluate([valid_input_ids, valid_attention_masks], valid_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJEXwbmvdxpn",
        "outputId": "7311ff9d-7e80-4be8-a1cc-a89464793269"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 108ms/step - loss: 0.6494 - accuracy: 0.6667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6493884921073914, 0.6666666865348816]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 평가하기\n",
        "a = model.predict([valid_input_ids, valid_attention_masks])\n",
        "a = tf.squeeze(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wUSVPOjlrEa",
        "outputId": "519496bc-c13b-468e-cefb-42f3aa28ef30"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 0s 113ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = np.array(a)\n",
        "b = np.rint(b).astype('int32')\n",
        "print(a[30:50])\n",
        "print(b[30:50])\n",
        "print(valid_labels[30:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oExquTWal_Qx",
        "outputId": "5d5031ad-a284-418c-8a9f-d42f9f560a2d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[0.41292006 0.9120563  0.15600671 0.13988551 0.9508612  0.50196445\n",
            " 0.18293615 0.9213992  0.76509213 0.89244586 0.91780096 0.18952268\n",
            " 0.68899274 0.15414175 0.7173258  0.8037388  0.92591494 0.21547012\n",
            " 0.8951298  0.46448013], shape=(20,), dtype=float32)\n",
            "[0 1 0 0 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0]\n",
            "[0 1 0 0 1 0 0 0 1 1 1 1 1 0 0 1 1 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_titles[30:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPjNHaoqmLHR",
        "outputId": "24229df2-bf8d-445c-ead7-581190e47a92"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['한번에 제압하는 강형욱 ㄷㄷ 강형욱 개는훌륭하다 shorts', '아빠는 개 싫다',\n",
              "       '용산에 개 풀겠다 육견협회 개 식용 금지법 반대 집회서 경찰과 대치 현장영상 KBS 2023 11 30',\n",
              "       '동물극장 유튜브 오리지널 60점 만점에 58점 받은 천재견 사람과 깊은 교감을 나누는 마야네 5가족 동물극장 단짝 EP45 KBS 230107 방송',\n",
              "       '요크셔테리어 장난치기 yorkie', '길에서 생고생한 강아지 영암 유기견 보호소',\n",
              "       '와글와글 보더콜리와 허스키 목줄 놓으면 어떤 반응 보일까 2022 12 13 뉴스투데이 MBC',\n",
              "       '뭐든 먹어야 산다 시고르자브종 반려견 시골강아지',\n",
              "       'Every morning I didn 39 t know my dog 39 s behavior',\n",
              "       '강아지 키우고 싶을 때 보면 치료 안 됨 새끼 진돗개', '눈 꺼풀 이 무곱댜 미용중에 조는 미니비숑',\n",
              "       '웰시코기 8마리가 잠실 석촌호수에 나타나면 생기는 일 ㅋㅋㅋ', '덤벼 덤벼 진돗개서열 백안하우스풍뢰 진돗개 풍산개',\n",
              "       '돈가스 먹으러 가는 줄 알았는데 주사 맞은 리트리버 반응ㅋㅋㅋ Mom Dog s Behavior After Giving Birth to 8 Retriever Pups',\n",
              "       '릴스 1380만 조회수 월드스타 요크셔테리어 미용', '썰매견 사모예드를 눈밭에 풀어주었습니다',\n",
              "       '비숑 산책 후 비숑타임',\n",
              "       '시츄가 많으면 할 수 있는 것 1 The Shih Tzus 39 Tide Is Flowing What You Can Do With Lots Of Shih Tzus EP1',\n",
              "       '보더콜리가 새끼양을 몰자 어미양이', '웰시코기 꼬리의 슬픈 비밀'], dtype='<U94')"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Saving\n",
        "model.save(\"bert_model_v1.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15_Xti4QmP2C",
        "outputId": "49ce4009-1f1d-4e7c-dc68-78bc16e5ecb5"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 다운로드 받기\n",
        "from google.colab import files\n",
        "\n",
        "files.download('bert_model_v1.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Naf83d_CmcHr",
        "outputId": "9f653f93-c98c-45ee-fc1f-e976bbc9c673"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_98bd416d-926a-4d38-9a7c-eebad78c6216\", \"bert_model_v1.h5\", 2135380720)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InOONY2LmwbB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}